#!/bin/bash

#SBATCH --output=archive/exp1/1KGP_chr22_ALL_seg128_overlap16/test_%A.log
#SBATCH --time=0-23:30:00
#SBATCH --job-name=1kgp
#SBATCH --account=your_hpc_account

#SBATCH --partition=your_hpc_partition
#SBATCH --nodes=10
#SBATCH --ntasks=150
#SBATCH --ntasks-per-node=15
#SBATCH --cpus-per-task=4

# #SBATCH --partition=your_hpc_partition
# #SBATCH --nodes=8
# #SBATCH --ntasks=16
# #SBATCH --ntasks-per-node=2
# #SBATCH --cpus-per-task=30
# #SBATCH --gpus-per-task=1
# #SBATCH --gres=gpu:2

# Setup environment
source $HOME/miniconda3/etc/profile.d/conda.sh
conda activate $HOME/miniconda3/envs/u19

# Set common environment variables for PyTorch Distributed
export WORLD_SIZE=$SLURM_NTASKS
export RANK=$SLURM_PROCID

# Set up master address and port with node coordination
# Determine master node and set it as the master address
MASTER_NODE=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
MASTER_ADDR=$(srun --nodes=1 --ntasks=1 -w $MASTER_NODE hostname --ip-address | tail -n 1)

# Generate a unique port based on job ID
MASTER_PORT=$(( 10000 + $(($SLURM_JOB_ID % 50000)) ))

# Export variables for all processes
export MASTER_ADDR=$MASTER_ADDR
export MASTER_PORT=$MASTER_PORT

CONFIG_FILE="archive/exp1/1KGP_chr22_ALL_seg128_overlap16/LONI_Feb25_2304.yaml"

start_time=$(date +%s)
echo "SLURM Job ID: $SLURM_JOB_ID | Array Task ID: $SLURM_ARRAY_TASK_ID | Started at: $(date)"

# Run the python script with SLURM array task ID as missing index
srun --export=ALL python -u test.py --configFile $CONFIG_FILE

end_time=$(date +%s)
elapsed_time=$((end_time - start_time))
elapsed_minutes=$((elapsed_time / 60))
elapsed_seconds=$((elapsed_time % 60))
echo "SLURM Job ID: $SLURM_JOB_ID | Array Task ID: $SLURM_ARRAY_TASK_ID | Finished at: $(date)"
echo "Total runtime: ${elapsed_minutes} min ${elapsed_seconds} sec"
